{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script assumes the corpus is located in `/data/corpus/`. The corpus can be downloaded using `get_corus.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Users...\n",
      "Loading conversations...\n"
     ]
    }
   ],
   "source": [
    "import wiki\n",
    "corpus = wiki.Corpus('../data/corpus/')\n",
    "utts = corpus.get_utts()\n",
    "users = {utt.user_id for utt in utts if utt.user_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User statistics:\n",
    "- Eigenvector centrality (+ binarized centrality)\n",
    "- avg. perplexity (of the user's utterances)\n",
    "- total number of utterances\n",
    "- Admin status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_stats_list = ['user_id', 'eigen_central', 'eigen_central_bin', 'avg_perplexity', 'n_utts', 'admin_status']\n",
    "user_stats = {user: {stat: None for stat in user_stats_list} for user in users}\n",
    "for user in user_stats:\n",
    "    user_stats[user]['user_id'] = user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating network from 389121 utterances...\n",
      "There were 111264 replies to unknown users.\n",
      "The unpruned network has  25813 nodes.\n",
      "Pruning network to its largest component...\n",
      "\t removed 34 users from 15 disconnected components.\n",
      "Normalizing edge weights...\n",
      "max weight 767\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy\n",
    "\n",
    "network = corpus.generate_network()\n",
    "centrality = nx.eigenvector_centrality_numpy(network, weight=None)\n",
    "\n",
    "mean = numpy.mean(centrality.values())\n",
    "stddev = numpy.std(centrality.values())\n",
    "\n",
    "for user in users:\n",
    "    eigen = centrality[user] if user in centrality else 0\n",
    "    user_stats[user]['eigen_central'] = eigen\n",
    "    user_stats[user]['eigen_central_bin'] = eigen > mean + stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Perplexity & Number of Utterances\n",
    "\n",
    "First, we set up the KenLM n-gram language model for the corpus: \n",
    "- [Download and install](https://kheafield.com/code/kenlm/)\n",
    "- [Paper](https://kheafield.com/papers/avenue/kenlm.pdf)\n",
    "- [Python module](https://github.com/kpu/kenlm)\n",
    "KenLM needs to be run from the command line to generate a language model object. KenLM expects to receive a corpus in [this](https://kheafield.com/code/kenlm/estimation/) format. The language model object can then be loaded into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/kenlm/build/bin/lmplz -o 3 -S 20% <../data/lm_corpus.txt >../data/lm.arpa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kenlm\n",
    "import os\n",
    "\n",
    "# Location of the compiled KenLM utility\n",
    "lmplz = '~/kenlm/build/bin/lmplz'\n",
    "# Location to store temporary input and output files for KenLM\n",
    "corpus_file = '../data/lm_corpus.txt'\n",
    "kenlm_file = '../data/lm.arpa'\n",
    "\n",
    "# Format the corpus for KenLM. See corpus formatting notes: https://kheafield.com/code/kenlm/estimation/\n",
    "kenlm_corpus = '\\n'.join(' '.join(b.tokenized) for b in utts)\n",
    "with open(corpus_file, 'w') as f:\n",
    "    f.write(kenlm_corpus)\n",
    "# use KenLM to create the n-gram language model\n",
    "print('{0} -o 3 -S 20% <{1} >{2}'.format(lmplz, corpus_file, kenlm_file))\n",
    "os.system('{0} -o 3 -S 20% <{1} >{2}'.format(lmplz, corpus_file, kenlm_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "lm = kenlm.Model(kenlm_file)\n",
    "utt_perplexity = defaultdict(list)\n",
    "\n",
    "for utt in utts:\n",
    "    utt_perplexity[utt.user_id].append(lm.perplexity(utt.clean_text))\n",
    "    \n",
    "for user in users:\n",
    "    user_stats[user]['avg_perplexity'] = numpy.mean(utt_perplexity[user])\n",
    "    user_stats[user]['n_utts'] = len(utt_perplexity[user])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admin Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    if user in corpus.users:\n",
    "        user_stats[user]['admin_status'] = corpus.users[user].admin\n",
    "    else:\n",
    "        user_stats[user]['admin_status'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../data/user_stats.csv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, user_stats_list)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(user_stats.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Utterance Statistics\n",
    "- perplexity of the utterance (aggregate LM / time period LM)\n",
    "- user Admin status\n",
    "- user network centrality\n",
    "- date of utt\n",
    "- months since first utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wiki\n",
    "import csv\n",
    "\n",
    "corpus = wiki.Corpus('../data/corpus/')\n",
    "utts = corpus.get_utts()\n",
    "\n",
    "with open ('../data/user_statistics.csv') as f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
