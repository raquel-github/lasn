{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script assumes the corpus is located in `/data/corpus/`. The corpus can be downloaded using `get_corus.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Users...\n",
      "Loading conversations...\n"
     ]
    }
   ],
   "source": [
    "import wiki\n",
    "corpus = wiki.Corpus('../data/corpus/')\n",
    "utts = corpus.get_utts()\n",
    "users = {utt.user_id for utt in utts if utt.user_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User statistics:\n",
    "- Eigenvector centrality (+ binarized centrality)\n",
    "- avg. perplexity (of the user's utterances)\n",
    "- total number of utterances\n",
    "- Admin status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_stats_list = ['user_id', 'eigen_central', 'eigen_central_bin', 'avg_perplexity', \n",
    "                   'n_utts', 'admin_status', 'n_content_words', 'n_tokens', 'n_types', \n",
    "                   'n_function_words']\n",
    "user_stats = {user: {stat: None for stat in user_stats_list} for user in users}\n",
    "for user in user_stats:\n",
    "    user_stats[user]['user_id'] = user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating network from 389121 utterances...\n",
      "There were 111264 replies to unknown users.\n",
      "The unpruned network has  25813 nodes.\n",
      "Pruning network to its largest component...\n",
      "\t removed 34 users from 15 disconnected components.\n",
      "Normalizing edge weights...\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy\n",
    "\n",
    "network = corpus.generate_network()\n",
    "centrality = nx.eigenvector_centrality_numpy(network, weight=None)\n",
    "\n",
    "mean = numpy.mean(centrality.values())\n",
    "stddev = numpy.std(centrality.values())\n",
    "\n",
    "for user in users:\n",
    "    eigen = centrality[user] if user in centrality else 0\n",
    "    user_stats[user]['eigen_central'] = eigen\n",
    "    user_stats[user]['eigen_central_bin'] = eigen > mean + stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Perplexity & Number of Utterances\n",
    "\n",
    "First, we set up the KenLM n-gram language model for the corpus: \n",
    "- [Download and install](https://kheafield.com/code/kenlm/)\n",
    "- [Paper](https://kheafield.com/papers/avenue/kenlm.pdf)\n",
    "- [Python module](https://github.com/kpu/kenlm)\n",
    "KenLM needs to be run from the command line to generate a language model object. KenLM expects to receive a corpus in [this](https://kheafield.com/code/kenlm/estimation/) format. The language model object can then be loaded into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/kenlm/build/bin/lmplz -o 3 -S 20% <../data/lm_corpus.txt >../data/lm.arpa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kenlm\n",
    "import os\n",
    "\n",
    "# Location of the compiled KenLM utility\n",
    "lmplz = '~/kenlm/build/bin/lmplz'\n",
    "# Location to store temporary input and output files for KenLM\n",
    "corpus_file = '../data/lm_corpus.txt'\n",
    "kenlm_file = '../data/lm.arpa'\n",
    "\n",
    "# Format the corpus for KenLM. See corpus formatting notes: https://kheafield.com/code/kenlm/estimation/\n",
    "kenlm_corpus = '\\n'.join(' '.join(b.tokenized) for b in utts)\n",
    "with open(corpus_file, 'w') as f:\n",
    "    f.write(kenlm_corpus)\n",
    "# use KenLM to create the n-gram language model\n",
    "print('{0} -o 3 -S 20% <{1} >{2}'.format(lmplz, corpus_file, kenlm_file))\n",
    "os.system('{0} -o 3 -S 20% <{1} >{2}'.format(lmplz, corpus_file, kenlm_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "lm = kenlm.Model(kenlm_file)\n",
    "utt_perplexity = defaultdict(list)\n",
    "\n",
    "for utt in utts:\n",
    "    utt_perplexity[utt.user_id].append(lm.perplexity(utt.clean_text))\n",
    "    \n",
    "for user in users:\n",
    "    user_stats[user]['avg_perplexity'] = numpy.mean(utt_perplexity[user])\n",
    "    user_stats[user]['n_utts'] = len(utt_perplexity[user])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admin status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    if user in corpus.users:\n",
    "        user_stats[user]['admin_status'] = corpus.users[user].admin\n",
    "    else:\n",
    "        user_stats[user]['admin_status'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic style features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380000\r"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "markers = {m: wiki.get_lines(wiki.FWORDS_DIR + m + '.txt') for m in wiki.markers} \n",
    "\n",
    "type_counts = defaultdict(Counter)\n",
    "function_word_counts = defaultdict(lambda: {m: Counter({w: 0 for w in markers[m]}) for m in markers})\n",
    "content_word_count = Counter()\n",
    "\n",
    "for i,utt in enumerate(utts):\n",
    "    if i % 10000 == 0:\n",
    "        print(i, end = '\\r')\n",
    "    type_counts[utt.user_id].update(utt.tokenized)\n",
    "    for t in utt.tokenized:\n",
    "        content_word = True\n",
    "        for m in markers:\n",
    "            if t in markers[m]:\n",
    "                function_word_counts[utt.user_id][m][t] += 1\n",
    "                content_word = False\n",
    "        if content_word:\n",
    "            content_word_count[utt.user_id] += 1\n",
    "overall_function_word_counts = {m: sum([function_word_counts[user][m] for user in users], Counter()) for m in markers}\n",
    "\n",
    "for user in users:\n",
    "    user_stats[user]['n_types'] = len(type_counts[user])\n",
    "    user_stats[user]['n_tokens'] = sum(type_counts[user].values())\n",
    "    user_stats[user]['n_content_words'] = content_word_count[user]\n",
    "    user_stats[user]['n_function_words'] = sum([sum(function_word_counts[user][f].values()) for f in function_word_counts[user]])\n",
    "    # TODO: calculate function word distribution typicality for each marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../data/user_stats.csv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, user_stats_list)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(user_stats.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal utterance statistics\n",
    "- perplexity based on LM from that year\n",
    "- perplexity based on overall corpus LM\n",
    "- user Admin status at time of utterance\n",
    "- user network centrality at time of utterance\n",
    "- date of utt\n",
    "- months since first utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Users...\n",
      "Loading conversations...\n"
     ]
    }
   ],
   "source": [
    "import wiki\n",
    "\n",
    "corpus = wiki.Corpus('../data/corpus/')\n",
    "utts = corpus.get_utts()\n",
    "users = {utt.user_id for utt in utts if utt.user_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build timeboxed user networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating network from 46191 utterances...\n",
      "There were 15474 replies to unknown users.\n",
      "The unpruned network has  5883 nodes.\n",
      "Pruning network to its largest component...\n",
      "\t removed 143 users from 64 disconnected components.\n",
      "Normalizing edge weights...\n",
      "Generating network from 58586 utterances...\n",
      "There were 19237 replies to unknown users.\n",
      "The unpruned network has  7024 nodes.\n",
      "Pruning network to its largest component...\n",
      "\t removed 170 users from 71 disconnected components.\n",
      "Normalizing edge weights...\n",
      "Generating network from 56579 utterances...\n",
      "There were 17440 replies to unknown users.\n",
      "The unpruned network has  6962 nodes.\n",
      "Pruning network to its largest component...\n",
      "\t removed 213 users from 99 disconnected components.\n",
      "Normalizing edge weights...\n",
      "Generating network from 54344 utterances...\n",
      "There were 15728 replies to unknown users.\n",
      "The unpruned network has  6998 nodes.\n",
      "Pruning network to its largest component...\n",
      "\t removed 293 users from 128 disconnected components.\n",
      "Normalizing edge weights...\n",
      "Generating network from 66399 utterances...\n",
      "There were 16707 replies to unknown users.\n",
      "The unpruned network has  7680 nodes.\n",
      "Pruning network to its largest component...\n",
      "\t removed 218 users from 98 disconnected components.\n",
      "Normalizing edge weights...\n",
      "Generating network from 92896 utterances...\n",
      "There were 22288 replies to unknown users.\n",
      "The unpruned network has  8827 nodes.\n",
      "Pruning network to its largest component...\n",
      "\t removed 225 users from 101 disconnected components.\n",
      "Normalizing edge weights...\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy\n",
    "import datetime\n",
    "\n",
    "eigen_centr = {year:{} for year in range(2006,2012)}\n",
    "eigen_centr_bin = {year:{} for year in range(2006,2012)}\n",
    "\n",
    "\n",
    "for year in range(2006, 2012):\n",
    "    \n",
    "    start_date = datetime.datetime(year,1,1)\n",
    "    end_date = datetime.datetime(year,12,31)\n",
    "    \n",
    "    network = corpus.generate_network(start_date=start_date, end_date=end_date)\n",
    "    centrality = nx.eigenvector_centrality_numpy(network, weight=None)\n",
    "\n",
    "    mean = numpy.mean(centrality.values())\n",
    "    stddev = numpy.std(centrality.values())\n",
    "\n",
    "    eigen_centr[year] = {user: centrality[user] if user in centrality else 0 for user in users}\n",
    "    eigen_centr_bin[year] = {user: eigen_centr[year][user] > mean + stddev for user in users}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building timeboxed language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/kenlm/build/bin/lmplz -o 3 -S 20% <../data/lm_corpus.txt >../data/lm_2006.arpa\n",
      "~/kenlm/build/bin/lmplz -o 3 -S 20% <../data/lm_corpus.txt >../data/lm_2007.arpa\n",
      "~/kenlm/build/bin/lmplz -o 3 -S 20% <../data/lm_corpus.txt >../data/lm_2008.arpa\n",
      "~/kenlm/build/bin/lmplz -o 3 -S 20% <../data/lm_corpus.txt >../data/lm_2009.arpa\n",
      "~/kenlm/build/bin/lmplz -o 3 -S 20% <../data/lm_corpus.txt >../data/lm_2010.arpa\n",
      "~/kenlm/build/bin/lmplz -o 3 -S 20% <../data/lm_corpus.txt >../data/lm_2011.arpa\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "import os\n",
    "\n",
    "lm = {}\n",
    "\n",
    "for year in range(2006, 2012):\n",
    "    # Location of the compiled KenLM utility\n",
    "    lmplz = '~/kenlm/build/bin/lmplz'\n",
    "    # Location to store temporary input and output files for KenLM\n",
    "    corpus_file = '../data/lm_corpus.txt'\n",
    "    kenlm_file = '../data/lm_{0}.arpa'.format(year)\n",
    "\n",
    "    # Format the corpus for KenLM. See corpus formatting notes: https://kheafield.com/code/kenlm/estimation/\n",
    "    kenlm_corpus = '\\n'.join(' '.join(b.tokenized) for b in [utt for utt in utts if utt.timestamp.year == year])\n",
    "    with open(corpus_file, 'w') as f:\n",
    "        f.write(kenlm_corpus)\n",
    "    # use KenLM to create the n-gram language model\n",
    "    print('{0} -o 3 -S 20% <{1} >{2}'.format(lmplz, corpus_file, kenlm_file))\n",
    "    os.system('{0} -o 3 -S 20% <{1} >{2}'.format(lmplz, corpus_file, kenlm_file))\n",
    "    \n",
    "    lm[year] = kenlm.Model(kenlm_file)\n",
    "    \n",
    "lm_all_years = kenlm.Model('../data/lm.arpa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utt_stats_list = ['utt_id', 'timestamp', 'perplexity_overall', 'perplexity_year',\n",
    "                  'admin_status', 'eigen_central', 'eigen_central_bin',\n",
    "                  'utt_rank', 'days_since_first', 'utt_length']\n",
    "utt_stats = {}\n",
    "\n",
    "max_date = datetime.datetime(2011,12,31)\n",
    "min_date = datetime.datetime(2006,1,1)\n",
    "\n",
    "for utt in utts:\n",
    "    \n",
    "    if not utt.timestamp or utt.timestamp < min_date or utt.timestamp > max_date:\n",
    "        continue\n",
    "        \n",
    "    utt_id = utt.utt_id\n",
    "    user_id = utt.user_id\n",
    "    year = utt.timestamp.year\n",
    "    utt_stats[utt_id] = {}\n",
    "\n",
    "    utt_stats[utt_id]['utt_id'] = utt_id\n",
    "    utt_stats[utt_id]['timestamp'] = utt.timestamp.isoformat()\n",
    "    \n",
    "    utt_stats[utt_id]['perplexity_overall'] = lm_all_years.perplexity(utt.clean_text)\n",
    "    utt_stats[utt_id]['perplexity_year'] = lm[year].perplexity(utt.clean_text)\n",
    "    \n",
    "    utt_stats[utt_id]['eigen_central'] = eigen_centr[year][user_id] if user_id in eigen_centr[year] else 0\n",
    "    utt_stats[utt_id]['eigen_central_bin'] = eigen_centr_bin[year][user_id] if user_id in eigen_centr_bin[year] else False\n",
    "    utt_stats[utt_id]['utt_length'] = len(utt.clean_text.split())\n",
    "    \n",
    "    if not user_id in corpus.users:\n",
    "        utt_stats[utt_id]['admin_status'] = False\n",
    "    elif corpus.users[user_id].admin:\n",
    "        ascention = corpus.users[user_id].admin_ascention\n",
    "        if not ascention or ascention <= utt.timestamp:\n",
    "            utt_stats[utt_id]['admin_status'] = True\n",
    "        else:\n",
    "            utt_stats[utt_id]['admin_status'] = False\n",
    "    else:\n",
    "        utt_stats[utt_id]['admin_status'] = False\n",
    "        \n",
    "    if not user_id in corpus.users:\n",
    "        utt_stats[utt_id]['utt_rank'] = 0\n",
    "        utt_stats[utt_id]['days_since_first'] = 0\n",
    "    else:\n",
    "        user_utts = corpus.users[user_id].utts\n",
    "        utt_stats[utt_id]['utt_rank'] = len([utt2 for utt2 in user_utts if utt2.timestamp < utt.timestamp])\n",
    "        utt_stats[utt_id]['days_since_first'] = (utt.timestamp - min([utt2.timestamp for utt2 in user_utts])).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../data/utt_stats.csv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, utt_stats_list)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(utt_stats.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
