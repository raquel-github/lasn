{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to assess what effect social network centrality has on a speaker's influence on the overall speech pattern of a language community. To do this we will:\n",
    "\n",
    "1. Break the corpus up into year-long periods\n",
    "2. Create a social network for each period\n",
    "3. Measure users' centrality in the social network\n",
    "2. Create a language model for each period\n",
    "3. Measure the average [perplexity](https://en.wikipedia.org/wiki/Perplexity#Perplexity_per_word) of a user's speech for each period\n",
    "4. Correlate centrality with perplexity\n",
    "  1. Expect to see the highest negative correlation between centrality and perplexity in the following period\n",
    "  2. Check this against correlation between centrality and perplexity in the current period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import wiki\n",
    "import kenlm\n",
    "import datetime\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Wiki corpus. This assumes the corpus is located in `/data/corpus/`. The corpus can be downloaded using `get_corus.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = wiki.Corpus('../data/corpus/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use KenLM to build an n-gram model for each period.\n",
    "- [Download and install](https://kheafield.com/code/kenlm/)\n",
    "- [Paper](https://kheafield.com/papers/avenue/kenlm.pdf)\n",
    "- [Python module](https://github.com/kpu/kenlm)\n",
    "KenLM needs to be run from the command line to generate a language model object. KenLM expects to receive a corpus in [this](https://kheafield.com/code/kenlm/estimation/) format. The language model object can then be loaded into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the compiled KenLM utility\n",
    "lmplz = '~/kenlm/build/bin/lmplz'\n",
    "# Location to store temporary input and output files for KenLM\n",
    "corpus_file = '../data/lm_corpus_{0}.txt'\n",
    "kenlm_file = '../data/lm_{0}.arpa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create the language model for each period (using periods of 1 year here).\n",
    "Further utterance cleanup needed:\n",
    "\n",
    "- remove URLs: `[www.google.com]` --> `''`\n",
    "- strp URL from links: `[www.google.com|Google]` --> `Google`\n",
    "- dereference Wikipedia links: `[[The New York Times]]` --> `The New York Times`\n",
    "- remove unencoded unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2006,2012):\n",
    "    \n",
    "    start_date = datetime.datetime(year, 1, 1)\n",
    "    end_date = datetime.datetime(year, 12, 31)\n",
    "    \n",
    "    utts = corpus.get_utts(start_date=start_date, end_date=end_date)\n",
    "    users = {utt.user_id: None for utt in utts}\n",
    "\n",
    "    # Create the corpus for this year in the appropriate format for KenLM. See corpus formatting notes: https://kheafield.com/code/kenlm/estimation/\n",
    "    kenlm_corpus = '\\n'.join(' '.join(b.tokenized) for b in utts)\n",
    "    with open(corpus_file, 'w') as f:\n",
    "        f.write(kenlm_corpus)\n",
    "    # use KenLM to create the n-gram language model for this year\n",
    "    os.system('{0} -o 3 -S 20% <{1} >{2}'.format(lmplz, corpus_file.format(year), kenlm_file.format(year)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we gather data for each period...\n",
    "\n",
    "1. Number of utterances by user\n",
    "2. Network centrality by user\n",
    "3. Average perplexity of users' utterances based on that period's language model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "('Generating network from', 46191, 'utterances...')\n",
      "('There were', 15474, 'replies to unknown users.')\n",
      "('The unpruned network has ', 5883, 'nodes.')\n",
      "Pruning network to its largest component...\n",
      "('\\t removed', 143, 'users from', 64, 'disconnected components.')\n",
      "Normalizing edge weights...\n",
      "2007\n",
      "('Generating network from', 58586, 'utterances...')\n",
      "('There were', 19237, 'replies to unknown users.')\n",
      "('The unpruned network has ', 7024, 'nodes.')\n",
      "Pruning network to its largest component...\n",
      "('\\t removed', 170, 'users from', 71, 'disconnected components.')\n",
      "Normalizing edge weights...\n",
      "2008\n",
      "('Generating network from', 56579, 'utterances...')\n",
      "('There were', 17440, 'replies to unknown users.')\n",
      "('The unpruned network has ', 6962, 'nodes.')\n",
      "Pruning network to its largest component...\n",
      "('\\t removed', 213, 'users from', 99, 'disconnected components.')\n",
      "Normalizing edge weights...\n",
      "2009\n",
      "('Generating network from', 54344, 'utterances...')\n",
      "('There were', 15728, 'replies to unknown users.')\n",
      "('The unpruned network has ', 6998, 'nodes.')\n",
      "Pruning network to its largest component...\n",
      "('\\t removed', 293, 'users from', 128, 'disconnected components.')\n",
      "Normalizing edge weights...\n",
      "2010\n",
      "('Generating network from', 66399, 'utterances...')\n",
      "('There were', 16707, 'replies to unknown users.')\n",
      "('The unpruned network has ', 7680, 'nodes.')\n",
      "Pruning network to its largest component...\n",
      "('\\t removed', 218, 'users from', 98, 'disconnected components.')\n",
      "Normalizing edge weights...\n",
      "2011\n",
      "('Generating network from', 92896, 'utterances...')\n",
      "('There were', 22288, 'replies to unknown users.')\n",
      "('The unpruned network has ', 8827, 'nodes.')\n",
      "Pruning network to its largest component...\n",
      "('\\t removed', 225, 'users from', 101, 'disconnected components.')\n",
      "Normalizing edge weights...\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for year in range(2006,2012):\n",
    "    print(year)\n",
    "    start_date = datetime.datetime(year, 1, 1)\n",
    "    end_date = datetime.datetime(year, 12, 31)\n",
    "    \n",
    "    model = kenlm.Model(kenlm_file.format(year))\n",
    "\n",
    "    network = corpus.generate_network(start_date=start_date, end_date=end_date)\n",
    "    centrality = nx.closeness_centrality(network)\n",
    "    \n",
    "    year_data = {}\n",
    "    for user in network:\n",
    "        user_utts = {' '.join(utt.tokenized) for utt in corpus.get_utts(user, start_date, end_date)}\n",
    "        n_utts = len(user_utts)\n",
    "        avg_perplexity = scipy.mean([model.perplexity(utt) for utt in user_utts])\n",
    "        index = user + '-' + str(year)  \n",
    "        year_data[user ] = {'n_utts': n_utts, \n",
    "                           'avg_perplexity': avg_perplexity, \n",
    "                           'centrality': centrality[user]}\n",
    "         \n",
    "    data[year] = pd.DataFrame.from_dict(year_data, orient='index')\n",
    "panel = pd.Panel(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "#utts <> perplexty -0.00848778352041\n",
      "Centrality <> Perplexity -0.0457800953046\n",
      "centrality <> next year perplexity -0.0231191578568\n",
      "\n",
      "2007\n",
      "#utts <> perplexty -0.00608223952797\n",
      "Centrality <> Perplexity -0.0549031939729\n",
      "centrality <> next year perplexity -0.0530200005688\n",
      "\n",
      "2008\n",
      "#utts <> perplexty -0.00933152555147\n",
      "Centrality <> Perplexity -0.0360035216956\n",
      "centrality <> next year perplexity -0.0305712133347\n",
      "\n",
      "2009\n",
      "#utts <> perplexty -0.010928106278\n",
      "Centrality <> Perplexity -0.0421389554152\n",
      "centrality <> next year perplexity -0.061451343082\n",
      "\n",
      "2010\n",
      "#utts <> perplexty -0.0139684027976\n",
      "Centrality <> Perplexity -0.0525567776622\n",
      "centrality <> next year perplexity -0.0468398856555\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for year in range(2006,2011):\n",
    "    corrs = panel[year].join(panel[year+1], lsuffix=str(year), rsuffix=str(year+1)).corr()\n",
    "    print(year)\n",
    "    print('#utts <> perplexty', corrs['n_utts'+str(year)]['avg_perplexity'+str(year)])\n",
    "    print('Centrality <> Perplexity', corrs['centrality'+str(year)]['avg_perplexity'+str(year)])\n",
    "    print('centrality <> next year perplexity', corrs['centrality'+str(year)]['avg_perplexity'+str(year+1)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Year | #utts <> perplexty | Centrality <> Perplexity | centrality <> next year perplexity |\n",
    "|------|--------------------|--------------------------|------------------------------------|\n",
    "| 2006 | -0.009             | -0.046                   | -0.023                             |\n",
    "| 2007 | -0.006             | -0.055                   | -0.053                             |\n",
    "| 2008 | -0.009             | -0.036                   | -0.031                             |\n",
    "| 2009 | -0.011             | -0.042                   | -0.061                             |\n",
    "| 2010 | -0.014             | -0.053                   | -0.047                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
