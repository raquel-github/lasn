{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26397/26397 [00:00<00:00, 161235.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening corpus files...\n",
      "Loading users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading posts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 516766/516766 [00:09<00:00, 52634.84it/s]\n",
      "  0%|          | 122/389121 [00:00<05:20, 1214.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 389121/389121 [05:38<00:00, 1151.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import wiki\n",
    "corpus = wiki.WikiCorpus.from_corpus_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Tokenizer:                                                                                           \n",
    "        \n",
    "    END_OF_TEXT = '<END OF TEXT>'\n",
    "    UNKNOWN_TOKEN = '<UNKNOWN TOKEN>'\n",
    "    PAD_TOKEN = '<PAD TOKEN>'\n",
    "        \n",
    "    def __init__(self, tokens):                                                                          \n",
    "        self._tokens = tokens                                                                            \n",
    "        self._index_map = {t: i for i, t in enumerate(self._tokens)}\n",
    "        self.num_tokens = len(self._tokens)\n",
    "        self.UNKNOWN_TOKEN_INDEX = self._index_map[Tokenizer.UNKNOWN_TOKEN]\n",
    "        self.END_OF_TEXT_INDEX = self._index_map[Tokenizer.END_OF_TEXT]\n",
    "        self.PAD_TOKEN_INDEX = self._index_map[Tokenizer.PAD_TOKEN]\n",
    "                                                                                                         \n",
    "    @classmethod                                                                                         \n",
    "    def fit(cls, texts, max_tokens):\n",
    "        print(\"Fitting tokenizer to texts...\")\n",
    "        token_count = Counter()\n",
    "        for text in tqdm(texts):\n",
    "            for token in text:                                                                           \n",
    "                token_count[token] += 1\n",
    "        aux_tokens = [Tokenizer.END_OF_TEXT, Tokenizer.UNKNOWN_TOKEN, Tokenizer.PAD_TOKEN]\n",
    "        tokens = token_count.most_common(max_tokens - len(aux_tokens)) + aux_tokens \n",
    "        return cls(tokens)                                     \n",
    "                                                                                                         \n",
    "    def token_to_index(self, t):                                                                               \n",
    "        return self._index_map.get(t, self.UNKNOWN_TOKEN_INDEX)\n",
    "                                                                                                         \n",
    "    def index_to_token(self, i):                                                                               \n",
    "        return self._tokens[i]\n",
    "\n",
    "    \n",
    "    def encode_texts(self, texts, sequence_length):\n",
    "        data = []\n",
    "        for text in texts:\n",
    "            tensor = torch.LongTensor(sequence_length)\n",
    "            i = 0\n",
    "            text.append(Tokenizer.END_OF_TEXT)\n",
    "            for token in text:\n",
    "                tensor[i] = self.token_to_index(token)\n",
    "                i += 1\n",
    "                if i == sequence_length:\n",
    "                    data.append(tensor)\n",
    "                    tensor = torch.LongTensor(sequence_length)\n",
    "                    i = 0\n",
    "            for i in range(i, sequence_length):\n",
    "                tensor[i] = self.PAD_TOKEN_INDEX\n",
    "                \n",
    "                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "for i in range(i, 14):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3405/778242 [00:00<00:22, 34036.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer to texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 109013/778242 [00:02<00:18, 37130.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-30998a6690e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-a91406331d17>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(cls, texts, max_tokens)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0mtoken_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tokens\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_OF_TEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNKNOWN_TOKEN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "posts = [t for t in post.tokens for post in corpus.posts.values()]\n",
    "tokenizer = Tokenizer.fit(posts, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 10000                                                                                   \n",
    "embedding_dim = 200\n",
    "hidden_dim = 200\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "learning_rate = 0.005\n",
    "batch_size = 128 \n",
    "max_seq_len = 32\n",
    "\n",
    "data = tokenizer.encode_texts(posts)\n",
    "n_batches = data.size(0) // batch_size\n",
    "data = data.narrow(0, 0, n_batches * batch_size)\n",
    "data = data.view(batch_size, -1).t().contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def get_batch(source, i, evaluation=False):\n",
    "    seq_len = min(max_seq_len, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "    32    32    32  ...     32    32    32\n",
       "    32    32    31  ...     32    32    32\n",
       "    32    32    32  ...     32    32    32\n",
       "        ...          ⋱          ...       \n",
       "    32    32    32  ...     31    32    32\n",
       "    32    32    32  ...     32    32    32\n",
       "    32    32    32  ...     32    32    32\n",
       " [torch.LongTensor of size 32x128], Variable containing:\n",
       "  32\n",
       "  32\n",
       "  31\n",
       "  ⋮ \n",
       "  32\n",
       "  32\n",
       "  32\n",
       " [torch.LongTensor of size 4096])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   32    32    32  ...     32    32    32\n",
       "   32    32    31  ...     32    32    32\n",
       "   32    32    32  ...     32    32    32\n",
       "       ...          ⋱          ...       \n",
       "   32    32    32  ...     32    32    32\n",
       "   32    32    32  ...     32    32    32\n",
       "   32    32    32  ...     32    32    32\n",
       "[torch.LongTensor of size 10x128]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
